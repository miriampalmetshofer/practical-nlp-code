{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of N-Grams\n",
    "\n",
    "One hot encoding, BoW and TF-IDF treat words as independent units. There is no notion of phrases or word ordering. Bag of N-grams (BoN) approach tries to remedy this. It does so by breaking text into chunks of n contiguous words/tokens. This can help us capture some context, which earlier approaches could not do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['often machine learning tutorials will recommend or require that you prepare your data in specific ways before fitting a machine learning model',\n",
       " 'getting started in applied machine learning can be difficult especially when working with real world data',\n",
       " 'one good example is to use a one hot encoding on categorical data']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our corpus\n",
    "documents = [\n",
    "    \"Often, machine learning tutorials will recommend or require that you prepare your data in specific ways before fitting a machine learning model.\",\n",
    "    \"Getting started in applied machine learning can be difficult, especially when working with real-world data.\",\n",
    "    \"One good example is to use a one-hot encoding on categorical data.\"\n",
    "]\n",
    "\n",
    "processed_docs = [doc.lower().replace(\",\", \"\").replace(\".\", \"\").replace(\"-\", \" \") for doc in documents]\n",
    "processed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer, which we used for BoW, can be used for getting a Bag of N-grams representation as well, using its ngram_range argument. The code snippet below shows how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our vocabulary:  {'often': 40, 'machine': 37, 'learning': 33, 'tutorials': 65, 'will': 73, 'recommend': 53, 'or': 47, 'require': 55, 'that': 61, 'you': 81, 'prepare': 49, 'your': 83, 'data': 10, 'in': 28, 'specific': 57, 'ways': 69, 'before': 4, 'fitting': 20, 'model': 39, 'often machine': 41, 'machine learning': 38, 'learning tutorials': 36, 'tutorials will': 66, 'will recommend': 74, 'recommend or': 54, 'or require': 48, 'require that': 56, 'that you': 62, 'you prepare': 82, 'prepare your': 50, 'your data': 84, 'data in': 11, 'in specific': 30, 'specific ways': 58, 'ways before': 70, 'before fitting': 5, 'fitting machine': 21, 'learning model': 35, 'getting': 22, 'started': 59, 'applied': 0, 'can': 6, 'be': 2, 'difficult': 12, 'especially': 16, 'when': 71, 'working': 77, 'with': 75, 'real': 51, 'world': 79, 'getting started': 23, 'started in': 60, 'in applied': 29, 'applied machine': 1, 'learning can': 34, 'can be': 7, 'be difficult': 3, 'difficult especially': 13, 'especially when': 17, 'when working': 72, 'working with': 78, 'with real': 76, 'real world': 52, 'world data': 80, 'one': 44, 'good': 24, 'example': 18, 'is': 31, 'to': 63, 'use': 67, 'hot': 26, 'encoding': 14, 'on': 42, 'categorical': 8, 'one good': 45, 'good example': 25, 'example is': 19, 'is to': 32, 'to use': 64, 'use one': 68, 'one hot': 46, 'hot encoding': 27, 'encoding on': 15, 'on categorical': 43, 'categorical data': 9}\n",
      "BoW representation for document 1:  [[0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 2 0 1\n",
      "  1 2 2 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0\n",
      "  0 1 1 0 0 0 0 0 0 1 1 1 1]]\n",
      "BoW representation for document 2:  [[1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0\n",
      "  0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
      "  1 0 0 1 1 1 1 1 1 0 0 0 0]]\n",
      "BoW representation for 'machine learning is good': [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0\n",
      "  0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# N-gram vectorization example with count vectorizer and unigrams, bigrams\n",
    "count_vect = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "# Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "\n",
    "# Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)\n",
    "\n",
    "# See the BOW rep for first 2 documents\n",
    "print(\"BoW representation for document 1: \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for document 2: \", bow_rep[1].toarray())\n",
    "\n",
    "# Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"machine learning is good\"])\n",
    "\n",
    "print(\"BoW representation for 'machine learning is good':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the number of features (and hence the size of the feature vector) increased a lot for the same data, compared to the other single word based representations!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
